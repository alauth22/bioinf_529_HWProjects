{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIOINF529 Homework #3 - Winter 2022\n",
    "This homework is worth **10% of your final grade**.\n",
    "\n",
    "The exam is due before the next course module begins as enforced by Canvas.\n",
    "\n",
    "## Coding by Contract\n",
    "We (the Instructors) promise a fair, impartial, and objective means of grading such that you (the Students) follow the tenets of Coding by Contract:\n",
    "1. You must not modify/delete any of the existing code in this document (besides the `pass` statements)\n",
    "* Your functions must use the function signatures as written\n",
    "* Your functions must return/print the expected results (as written)\n",
    "\n",
    "If these are followed correctly, your submission should be compatible with the automated testing suite. Therefore, the more tests your code passes, the less scrutiny your code will be under by our review. We do not care *how* you get there, just that you get there *correctly*.\n",
    "\n",
    "## Submission\n",
    "Please rename this notebook to **homework3_uniqname.ipynb** for submission. \n",
    "\n",
    "For example:\n",
    "> `homework3_apboyle.ipynb`\n",
    "\n",
    "We will *only* grade the most recent submission of your exam.\n",
    "\n",
    "## Late Policy\n",
    "Each submission will receive a **25%** penalty per day (up to three days) that the assignment is late.\n",
    "\n",
    "After that, the student will receive a **0** for the homework.\n",
    "\n",
    "## Academic Honor Code\n",
    "You may consult with others. However, all answers must be your own and code comparison software will be used to enforce this rule. You are allowed to ask questions at office hours but the answers given will be high-level/conceptual in nature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baum-Welch\n",
    "In class we have now implemented aspects of evaluating a hidden Markov model through Viterbi and the Forward, Backward, and Forward-Backward algorithms. Each of these algorithms requires prior knowledge of dataset labels in order to generate the state inititaion, transition, and emission probabilities. You will now implement the Baum-Welch algorithm which will learn these probabilities for your model in an unsupervised manner. For this assignement, you may use any of the previously provided class implementations as part of your submission.\n",
    "\n",
    "### The Baum-Welch Algorithm\n",
    "The Baum-Welch algorithm, much like the other algorithms in the last few classes, consists of a series of steps: Initialization, Expectation, Maximization, and Termination. It is a type of Expectation-Maximization algorithm and so you will loop between the Expectation and Maximization steps until you reach convergence.\n",
    "\n",
    "Initialization: <br>\n",
    "Set arbitrary parameters for $\\Theta$ in $A$ transition probabilities, $E$ emission probabilities, and $B$ starting probabilities. <br>\n",
    "\n",
    "Expectation:<br>\n",
    "For each input sequence $x$ where sequence index $j = 1 \\dots n $:<br>\n",
    "Calculate $f_{k}(i)$ matrix for sequence $x$ using the Forward algorithm.<br>\n",
    "Calculate $r_{k}(i)$ matrix for sequence $x$ using the Backward algorithm.<br>\n",
    "Update transition matrix $A_{kl}$ by summing over all positions ($i=1\\dots T-1$):<br>\n",
    "$A_{kl} = \\sum_{j}1/P(x^{j}) \\sum_{i}f_{k}^{j}(i)a_{kl}e_{l}(x_{i+1}^{j})r_{l}^{j}(i+1)$ <br>\n",
    "where $x^{j}$, $f^{j}$, and $r^{j}$ are sequence, forward matrix, and backward matrix for squence index $j$ respectively.<br>\n",
    "Update emission matrix $E_{k}$ by summing over all positions ($i=1\\dots T$):<br>\n",
    "$E_{k}(\\sigma) = \\sum_{j}1/P(x^{j}) \\sum_{i|x_{i}^{j}=\\sigma}f_{k}^{j}(i)r_{k}^{j}(i)$ <br>\n",
    "where the inner sum is only over positions $i$ that have emission $\\sigma$. <br>\n",
    "Update initial state matrix:<br>\n",
    "$B_{k} = \\sum_{j}1/P(x^{j}) * f_{k}^{j}(0)r_{k}^{j}(0)$\n",
    "\n",
    "Maximization:<br>\n",
    "Calculate new model parameters as we did with Markov Chains:<br>\n",
    "$a_{kl} = A_{kl}/\\sum_{l}A_{kl}$<br>\n",
    "$e_{k}(\\sigma) = E_{k}(\\sigma) / \\sum_{\\sigma}E_{k}(\\sigma)$<br>\n",
    "$b_{k} = B_{k} / \\sum_{k}{B_k}$\n",
    "\n",
    "<Br>Termination: <br>\n",
    "Stop at convergence as measured by log likelihood or is maximum number of iterations has been reached.\n",
    "\n",
    "Please add to the HMM class a new function baum_welch(self, sequences, pseudocount=1e-100) that takes as input a list of sequences to train the model. I have provided empty implentations of Viterbi, Forward, Backward, and Forward-Backward, but I recommend you place my solutions from class into these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \"\"\"Main class for HMM objects\n",
    "    \n",
    "    Class for holding HMM parameters and to allow for implementation of\n",
    "    functions associated with HMMs\n",
    "    \n",
    "    Private Attributes:\n",
    "        _alphabet (set): The alphabet of emissions\n",
    "        _hidden_states (set): Hidden states in the model\n",
    "        _transitions (dict(dict)): A dictionary of transition probabilities\n",
    "        _emissions (dict(dict)): A dictionary of emission probabilities\n",
    "        _initial (dict): A dictionary of initial state probabilities\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    __all__ = ['viterbi', 'forward', 'backward', 'forward_backward']\n",
    "\n",
    "    def __init__(self, alphabet, hidden_states, A=None, E=None, B=None, seed=None):\n",
    "        self._alphabet = set(alphabet)\n",
    "        self._hidden_states = set(hidden_states)\n",
    "        self._transitions = A\n",
    "        self._emissions = E\n",
    "        self._initial = B\n",
    "        self._seed = seed\n",
    "        if(self._transitions == None):\n",
    "            self._initialize_random(self._alphabet, self._hidden_states, self._seed)\n",
    "            \n",
    "    def __str__(self):\n",
    "        out_text = [f'Alphabet: {self._alphabet}',\n",
    "                    f'Hidden States: {self._hidden_states}',\n",
    "                    f'Initial Probabilities: {json.dumps(self._initial, sort_keys = True, indent=4)}',\n",
    "                    f'Transition Probabilities: {json.dumps(self._transitions, sort_keys = True, indent=4)}',\n",
    "                    f'Emission Probabilities: {json.dumps(self._emissions, sort_keys = True, indent=4)}']\n",
    "        return '\\n'.join(out_text)\n",
    "    \n",
    "    @classmethod\n",
    "    def __dir__(cls):\n",
    "        return cls.__all__\n",
    "    \n",
    "    def _emit(self, cur_state, symbol):\n",
    "        return self._emissions[cur_state][symbol]\n",
    "    \n",
    "    def _transition(self, cur_state, next_state):\n",
    "        return self._transitions[cur_state][next_state]\n",
    "    \n",
    "    def _init(self, cur_state):\n",
    "        return self._initial[cur_state]\n",
    "\n",
    "    def _states(self):\n",
    "        for k in self._hidden_states:\n",
    "            yield k\n",
    "    \n",
    "    \n",
    "    def _get_alphabet(self):\n",
    "        for sigma in self._alphabet:\n",
    "            yield sigma\n",
    "            \n",
    "    def _initialize_random(self, alphabet, states, seed):\n",
    "        alphabet = list(set(alphabet))\n",
    "        alphabet.sort()\n",
    "        states = list(set(states))\n",
    "        states.sort()\n",
    "        self._alphabet = alphabet\n",
    "        self._hidden_states = states\n",
    "\n",
    "        #Initialize empty matrices A and E with pseudocounts\n",
    "        A = {}\n",
    "        E = {}\n",
    "        I = {}\n",
    "        np.random.seed(seed=seed)\n",
    "        I_rand = np.random.dirichlet(np.ones(len(self._hidden_states)))\n",
    "        for i, state in enumerate(self._states()):\n",
    "            E[state] = {}\n",
    "            A[state] = {}\n",
    "            I[state] = I_rand[i]\n",
    "            E_rand = np.random.dirichlet(np.ones(len(self._alphabet)))\n",
    "            A_rand = np.random.dirichlet(np.ones(len(self._hidden_states)))\n",
    "            for j, sigma in enumerate(self._get_alphabet()):\n",
    "                E[state][sigma] = E_rand[j]\n",
    "            for j, next_state in enumerate(self._states()):\n",
    "                A[state][next_state] = A_rand[j]\n",
    "                \n",
    "        self._transitions = A\n",
    "        self._emissions = E\n",
    "        self._initial = I\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def viterbi(self, sequence):\n",
    "        \"\"\" The viterbi algorithm for decoding a string using a HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (list): optimal path through HMM given the model parameters\n",
    "                           using the Viterbi algorithm\n",
    "        \n",
    "        Pseudocode for Viterbi:\n",
    "            Initialization (ğ‘–=0): ğ‘£ğ‘˜(ğ‘–)=ğ‘’ğ‘˜(ğœ)ğ‘ğ‘˜.\n",
    "            Recursion (ğ‘–=1â€¦ğ‘‡): ğ‘£ğ‘™(ğ‘–)=ğ‘’ğ‘™(ğ‘¥ğ‘–) maxğ‘˜(ğ‘£ğ‘˜(ğ‘–âˆ’1)ğ‘ğ‘˜ğ‘™); \n",
    "                                ptrğ‘–(ğ‘™)= argmaxğ‘˜(ğ‘£ğ‘˜(ğ‘–âˆ’1)ğ‘ğ‘˜ğ‘™).\n",
    "            Termination: ğ‘ƒ(ğ‘¥,ğœ‹âˆ—)= maxğ‘˜(ğ‘£ğ‘˜(ğ‘™)ğ‘ğ‘˜0); \n",
    "                             ğœ‹âˆ—ğ‘™= argmaxğ‘˜(ğ‘£ğ‘˜(ğ‘™)ğ‘ğ‘˜0).\n",
    "            Traceback: (ğ‘–=ğ‘‡â€¦1): ğœ‹âˆ—ğ‘–âˆ’1= ptrğ‘–(ğœ‹âˆ—ğ‘–).\n",
    "        \"\"\"\n",
    "        #Taken from Alan's Class Solutions. \n",
    "        \n",
    "        # Initialization (ğ‘–=0): ğ‘£ğ‘˜(ğ‘–)=ğ‘’ğ‘˜(ğœ)ğ‘ğ‘˜.\n",
    "        # Initialize trellis and traceback matrices\n",
    "        # trellis will hold the vi data as defined by Durbin et al.\n",
    "        # and trackback will hold back pointers\n",
    "        trellis = {} # This only needs to keep the previous column probabilities\n",
    "        traceback = [] # This will need to hold all of the traceback data so will be an array of dicts()\n",
    "        for state in self._states():\n",
    "            trellis[state] = np.log10(self._init(state)) + np.log10(self._emit(state, sequence[0])) # b * e(0) for all k\n",
    "            \n",
    "        # Next we do the recursion step:\n",
    "        # Recursion (ğ‘–=1â€¦ğ‘‡): ğ‘£ğ‘™(ğ‘–)=ğ‘’ğ‘™(ğ‘¥ğ‘–) maxğ‘˜(ğ‘£ğ‘˜(ğ‘–âˆ’1)ğ‘ğ‘˜ğ‘™); \n",
    "        #                 ptrğ‘–(ğ‘™)= argmaxğ‘˜(ğ‘£ğ‘˜(ğ‘–âˆ’1)ğ‘ğ‘˜ğ‘™).\n",
    "        for t in range(1, len(sequence)):  # For each position in the sequence\n",
    "            trellis_next = {}\n",
    "            traceback_next = {}\n",
    "\n",
    "            for next_state in self._states():    # Calculate maxk and argmaxk\n",
    "                k={}\n",
    "                for cur_state in self._states():\n",
    "                    k[cur_state] = trellis[cur_state] + np.log10(self._transition(cur_state, next_state)) # k(t-1) * a\n",
    "                argmaxk = max(k, key=k.get)\n",
    "                trellis_next[next_state] =  np.log10(self._emit(next_state, sequence[t])) + k[argmaxk] # k * e(t)\n",
    "                traceback_next[next_state] = argmaxk\n",
    "                \n",
    "            #Overwrite trellis \n",
    "            trellis = trellis_next\n",
    "            #Keep trackback pointer matrix\n",
    "            traceback.append(traceback_next)\n",
    "            \n",
    "        # Termination: ğ‘ƒ(ğ‘¥,ğœ‹âˆ—)= maxğ‘˜(ğ‘£ğ‘˜(ğ‘™)ğ‘ğ‘˜0); \n",
    "        #                  ğœ‹âˆ—ğ‘™= argmaxğ‘˜(ğ‘£ğ‘˜(ğ‘™)ğ‘ğ‘˜0).\n",
    "        max_final_state = max(trellis, key=trellis.get)\n",
    "        max_final_prob = trellis[max_final_state]\n",
    "                \n",
    "        # Traceback: (ğ‘–=ğ‘‡â€¦1): ğœ‹âˆ—ğ‘–âˆ’1= ptrğ‘–(ğœ‹âˆ—ğ‘–).\n",
    "        result = [max_final_state]\n",
    "        for t in reversed(range(len(sequence)-1)):\n",
    "            result.append(traceback[t][max_final_state])\n",
    "            max_final_state = traceback[t][max_final_state]\n",
    "\n",
    "        return result[::-1]\n",
    "\n",
    "  \n",
    "\n",
    "    def forward(self, sequence):\n",
    "        \"\"\" The forward algorithm for calculating probability of sequence given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (float, list of dicts): P(x) and the f matrix as a list\n",
    "        \n",
    "        Pseudocode for Forward:\n",
    "            Initialization (ğ‘–=0): ğ‘“ğ‘˜(0)=ğ‘’ğ‘˜(ğœ0)ğ‘ğ‘˜.\n",
    "            Recursion (ğ‘–=1â€¦ğ‘‡): ğ‘“ğ‘™(ğ‘–)=ğ‘’ğ‘™(ğœğ‘–)âˆ‘ğ‘˜(ğ‘“ğ‘˜(ğ‘–âˆ’1)ğ‘ğ‘˜ğ‘™)\n",
    "            Termination: ğ‘ƒ(ğ‘¥)=âˆ‘ğ‘˜ğ‘“ğ‘˜(ğ‘‡)\n",
    "        \"\"\"\n",
    "        #Taken from Alan's Class Solutions. \n",
    "        \n",
    "        # Initialization (ğ‘–=0): ğ‘“ğ‘˜(0)=ğ‘’ğ‘˜(ğœ0)ğ‘ğ‘˜.\n",
    "        # Initialize f\n",
    "        f = [] # For this algorithm it is helpful to keep this entire matrix\n",
    "        f.append({})\n",
    "        for state in self._states():\n",
    "            f[-1][state] = self._init(state) * self._emit(state, sequence[0]) # b * e(0) for all k\n",
    "\n",
    "        # Next we do the recursion step:\n",
    "        # Recursion (ğ‘–=1â€¦ğ‘‡): ğ‘“ğ‘™(ğ‘–)=ğ‘’ğ‘™(ğœğ‘–)âˆ‘ğ‘˜(ğ‘“ğ‘˜(ğ‘–âˆ’1)ğ‘ğ‘˜ğ‘™) \n",
    "        for i in range(1, len(sequence)):  # For each position in the sequence\n",
    "            f.append({})\n",
    "            for next_state in self._states(): # For each state\n",
    "                f[-1][next_state] = 0\n",
    "                for cur_state in self._states():\n",
    "                    f[-1][next_state] += f[i-1][cur_state] * self._transition(cur_state, next_state) # sum of f(i-1) * a\n",
    "                f[-1][next_state] = self._emit(next_state, sequence[i]) * f[-1][next_state] # f * e(i)\n",
    "        \n",
    "        # Termination: ğ‘ƒ(ğ‘¥)=âˆ‘ğ‘˜ğ‘“ğ‘˜(ğ‘‡)\n",
    "        Px = 0\n",
    "        for state in self._states():\n",
    "            Px += f[-1][state]\n",
    "            \n",
    "        return Px, f\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, sequence):\n",
    "        \"\"\" The backward algorithm for calculating probability of sequence given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            result (float, list of dicts): P(x) and the b matrix as a list\n",
    "        \n",
    "        Pseudocode for Backward:\n",
    "            Initialization (ğ‘–=T): ğ‘Ÿğ‘˜(ğ‘‡)=1.\n",
    "            Recursion (ğ‘–=ğ‘‡âˆ’1â€¦1): ğ‘Ÿğ‘˜(ğ‘–)=âˆ‘ğ‘™ğ‘Ÿğ‘™(ğ‘–+1)ğ‘ğ‘˜ğ‘™ğ‘’ğ‘™(ğœğ‘–+1)\n",
    "            Termination: ğ‘ƒ(ğ‘¥)=âˆ‘ğ‘™ğ‘Ÿğ‘˜(1)ğ‘’ğ‘™(ğœ1)ğ‘ğ‘™\n",
    "        \"\"\"\n",
    "        #Taken from Alan's Class Solutions. \n",
    "        \n",
    "        # Initialization (ğ‘–=T): ğ‘Ÿğ‘˜(ğ‘‡)=1.\n",
    "        # Initialize r\n",
    "        r = [] # For this algorithm it is helpful to keep this entire matrix\n",
    "        r.insert(0, {})\n",
    "        for state in self._states():\n",
    "            r[0][state] = 1 # 1 for all k\n",
    "\n",
    "        # Next we do the recursion step:\n",
    "        # Recursion (ğ‘–=T-1â€¦1): ğ‘Ÿğ‘˜(ğ‘–)=âˆ‘ğ‘™ğ‘Ÿğ‘™(ğ‘–+1)ğ‘ğ‘˜ğ‘™ğ‘’ğ‘™(ğœğ‘–+1)\n",
    "        for i in range(len(sequence)-1, 0, -1):  # For each position in the sequence in reverse\n",
    "            r.insert(0, {}) # append a new item at the beginning\n",
    "            for prev_state in self._states(): # For each state\n",
    "                r[0][prev_state] = 0\n",
    "                for next_state in self._states():\n",
    "                    r[0][prev_state] += r[1][next_state] * self._transition(prev_state, next_state) * self._emit(next_state, sequence[i])\n",
    "\n",
    "        # Termination: ğ‘ƒ(ğ‘¥)=âˆ‘ğ‘™ğ‘Ÿğ‘˜(1)ğ‘’ğ‘™(ğœ1)ğ‘ğ‘™\n",
    "        Px = 0\n",
    "        for state in self._states():\n",
    "            Px += r[0][state] * self._init(state) * self._emit(state, sequence[0])\n",
    "                        \n",
    "        return Px, r\n",
    "    \n",
    "    \n",
    "    def forward_backward(self, sequence):\n",
    "        \"\"\" The forward-backward algorithm for calculating marginal posteriors given HMM\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of valid emissions from the HMM\n",
    "\n",
    "        Returns:\n",
    "            posterior (list of dicts): all posteriors as a list\n",
    "        \n",
    "        Pseudocode for Forward-Backward:\n",
    "            Calculate f[] as forward algorithm\n",
    "            Calculate r[] as backward algorithm\n",
    "            for all i in sequence\n",
    "                for all states\n",
    "                    posterior[i][state] = f[i][state] * r[i][state] / Px\n",
    "        \"\"\"    \n",
    "        #Taken from Alan's Class Solutions. \n",
    "        \n",
    "        #Calculate forward and backward matrices\n",
    "        f_Px, f_matrix = self.forward(sequence)\n",
    "        r_Px, r_matrix = self.backward(sequence)\n",
    "    \n",
    "        posterior = []\n",
    "        for i in range(0, len(sequence)):  # For each position in the sequence\n",
    "            posterior.append({})\n",
    "            for state in self._states(): # For each state\n",
    "                posterior[i][state] = f_matrix[i][state] * r_matrix[i][state] / f_Px\n",
    "                \n",
    "        return posterior\n",
    "    \n",
    "    \n",
    "    def baum_welch(self, sequences, pseudocount=1e-100):\n",
    "        \"\"\" The baum-welch algorithm for unsupervised HMM parameter learning\n",
    "\n",
    "        Args:\n",
    "            sequence (list): a list of sequences containing valid emissions from the HMM\n",
    "            pseudocount (float): small pseudocount value (default: 1e-100)\n",
    "\n",
    "        Returns:\n",
    "            None but updates the current HMM model parameters:\n",
    "             self._transitions, self._emissions, self._initial\n",
    "        \n",
    "        \"\"\"  \n",
    "        #Initialization Step:\n",
    "        #Set arbitrary parameters for alpha in A (transition), E (emission), and B (initial) prbabilitiess\n",
    "        \n",
    "        #initialize the empty dictionaries. \n",
    "        initial_prob = {}\n",
    "        transition_prob = {}\n",
    "        emission_prob = {}\n",
    "        \n",
    "        for current_state in self._hidden_states: #genome or island. \n",
    "            initial_prob[current_state] = pseudocount\n",
    "            #creating the nested dictionaries for the other ones. \n",
    "            transition_prob[current_state] = {}\n",
    "            emission_prob[current_state] = {}\n",
    "            #second for loops to fill in the nested dicts. \n",
    "            for next_state in self._hidden_states:\n",
    "                transition_prob[current_state][next_state] = pseudocount  \n",
    "            #take care of the A,C,G,T for the emission_prob matrix. \n",
    "            for alph in self._alphabet:\n",
    "                emission_prob[current_state][alph] = pseudocount\n",
    "        # print(initial_prob, transition_prob, emission_prob)\n",
    "\n",
    "        \n",
    "        #Expectation Step:\n",
    "        #For each input sequence  ğ‘¥  where sequence index  ğ‘—=1â€¦ğ‘› :\n",
    "        #Calculate  ğ‘“ğ‘˜(ğ‘–)  matrix for sequence  ğ‘¥  using the Forward algorithm.\n",
    "        #Calculate  ğ‘Ÿğ‘˜(ğ‘–)  matrix for sequence  ğ‘¥  using the Backward algorithm.\n",
    "        \n",
    "        #for convergence; decided to do a maximum number of iterations and chose the arbitrary number 1000 for iterations. \n",
    "        for i in range(0, 1000): #don't know the global max, but can find the local max (unsupervised ML) \n",
    "            Pxj = 0 #setting Pxj to 0 before beginning summation. \n",
    "            \n",
    "            #sequence in this function is a list of sequences. Need to iterate through each seq in the list. \n",
    "            for seq in sequences: \n",
    "                #want both outputs from the forward and reverse algorithms. \n",
    "                f_Px, f_matrix = self.forward(seq)\n",
    "                r_Px, r_matrix = self.backward(seq)\n",
    "                \n",
    "                #do this for every sequence and continually add. \n",
    "                Pxj += (f_Px + r_Px) / 2 \n",
    "                \n",
    "                #need to now update our probability matrices. \n",
    "                for index, holder in enumerate(seq):\n",
    "                    #note holder here is A,C,G,or T\n",
    "                    for state in self._hidden_states: #Island or Genome     \n",
    "                        if index == 0: #if at beginning \n",
    "                            # Update initial state matrix:\n",
    "                            # ğµğ‘˜=âˆ‘ğ‘—1/ğ‘ƒ(ğ‘¥ğ‘—)âˆ—ğ‘“ğ‘—ğ‘˜(0)ğ‘Ÿğ‘—ğ‘˜(0). \n",
    "                            initial_prob[state] += (f_matrix[index][state] * r_matrix[index][state])\n",
    "                            # print(initial_prob)\n",
    "                        if index != 0:\n",
    "                            #Update emission matrix  ğ¸ğ‘˜  by summing over all positions ( ğ‘–=1â€¦ğ‘‡ ):\n",
    "                            #ğ¸ğ‘˜(ğœ)=âˆ‘ğ‘—1/ğ‘ƒ(ğ‘¥ğ‘—)âˆ‘ğ‘–|ğ‘¥ğ‘—ğ‘–=ğœğ‘“ğ‘—ğ‘˜(ğ‘–)ğ‘Ÿğ‘—ğ‘˜(ğ‘–). \n",
    "                            emission_prob[state][holder] += (f_matrix[index][state] * r_matrix[index][state])       \n",
    "                        #when we reach the end of our seq.  \n",
    "                        if index == len(seq) - 1:               \n",
    "                            break #break out of for loop. \n",
    "                        \n",
    "                        #Update transition matrix  ğ´ğ‘˜ğ‘™  by summing over all positions ( ğ‘–=1â€¦ğ‘‡âˆ’1 ):\n",
    "                        #ğ´ğ‘˜ğ‘™=âˆ‘ğ‘—1/ğ‘ƒ(ğ‘¥ğ‘—)âˆ‘ğ‘–ğ‘“ğ‘—ğ‘˜(ğ‘–)ğ‘ğ‘˜ğ‘™ğ‘’ğ‘™(ğ‘¥ğ‘—ğ‘–+1)ğ‘Ÿğ‘—ğ‘™(ğ‘–+1). \n",
    "                        #where  ğ‘¥ğ‘— ,  ğ‘“ğ‘— , and  ğ‘Ÿğ‘—  are sequence, forward matrix, and backward matrix for squence index  ğ‘—  respectively.\n",
    "                        for next_state in self._hidden_states: #need next state. \n",
    "                            #I split this equation up onto separate lines since it was very long on ONE line. \n",
    "                            transition_prob[state][next_state] += (f_matrix[index][state]\n",
    "                                                                * self._transitions[state][next_state]\n",
    "                                                                * self._emissions[next_state][seq[index + 1]]\n",
    "                                                                * r_matrix[index + 1][next_state])\n",
    "             \n",
    "            \n",
    "            #want to continue to sum up the numerator and then divide by the one constant denomintor which is already summed.  \n",
    "            #for initial prob matrix.\n",
    "            for state in self._hidden_states: #I or G \n",
    "                initial_prob[state] = initial_prob[state] / Pxj\n",
    "                \n",
    "            #for emission prob matrix. \n",
    "            for state in self._hidden_states:\n",
    "                for alph in self._alphabet:\n",
    "                    emission_prob[state][alph] = emission_prob[state][alph] / Pxj \n",
    "                    \n",
    "            #for transition prob matrix. \n",
    "            for state in self._hidden_states:\n",
    "                for next_state in self._hidden_states:\n",
    "                    transition_prob[state][next_state] = transition_prob[state][next_state] / Pxj\n",
    "\n",
    "           \n",
    "            #Maximization Step:\n",
    "            #Calculate new model parameters as we did with Markov Chains:\n",
    "            \n",
    "            #getting all the summations or denominators for the maximization step. \n",
    "            #need to be out of the for seq in sequences loop for this to work. \n",
    "            sum_init = sum(initial_prob.values())\n",
    "            #this was very tricky since we want to only sum within the nested dictionary. \n",
    "            #only want the values here (integers). take the sum of values for each index. \n",
    "            #then divide each value at each index/state by that respective sum. \n",
    "            sum_emission = {state: sum(emission_prob[state].values()) for state in self._hidden_states}\n",
    "            sum_transition = {state: sum(transition_prob[state].values()) for state in self._hidden_states}\n",
    "        \n",
    "            #for initial prob matrix.\n",
    "            #ğ‘ğ‘˜=ğµğ‘˜/âˆ‘ğ‘˜ğµğ‘˜\n",
    "            for state in self._hidden_states:\n",
    "                initial_prob[state] = initial_prob[state] / sum_init \n",
    "            \n",
    "            #for emission prob matrix. \n",
    "            #ğ‘’ğ‘˜(ğœ)=ğ¸ğ‘˜(ğœ)/âˆ‘ğœğ¸ğ‘˜(ğœ)\n",
    "            for state in self._hidden_states:\n",
    "                for alph in self._alphabet:\n",
    "                    emission_prob[state][alph] = emission_prob[state][alph] / sum_emission[state]\n",
    "            \n",
    "            #for transition prob matrix. \n",
    "            #ğ‘ğ‘˜ğ‘™=ğ´ğ‘˜ğ‘™/âˆ‘ğ‘™ğ´ğ‘˜ğ‘™ \n",
    "            for state in self._hidden_states:\n",
    "                for next_state in self._hidden_states:\n",
    "                    transition_prob[state][next_state] = transition_prob[state][next_state] / sum_transition[state]\n",
    "        \n",
    "    \n",
    "        #Termination \n",
    "        #Stop at convergence as measured by log likelihood or is maximum number of iterations has been reached.  \n",
    "        #need .copy() to ensure we return the updated matrix. \n",
    "        self._initial = initial_prob.copy()\n",
    "        self._emissions = emission_prob.copy()\n",
    "        self._transitions = transition_prob.copy()\n",
    "        \n",
    "        #done\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphabet: ['A', 'C', 'G', 'T']\n",
      "Hidden States: ['G', 'I']\n",
      "Initial Probabilities: {\n",
      "    \"G\": 0.480482653962524,\n",
      "    \"I\": 0.519517346037476\n",
      "}\n",
      "Transition Probabilities: {\n",
      "    \"G\": {\n",
      "        \"G\": 0.6514755213356083,\n",
      "        \"I\": 0.3485244786643918\n",
      "    },\n",
      "    \"I\": {\n",
      "        \"G\": 0.6035452125544299,\n",
      "        \"I\": 0.39645478744557\n",
      "    }\n",
      "}\n",
      "Emission Probabilities: {\n",
      "    \"G\": {\n",
      "        \"A\": 0.33527124184256984,\n",
      "        \"C\": 0.2841112364196943,\n",
      "        \"G\": 0.1967270011087938,\n",
      "        \"T\": 0.18389052062894198\n",
      "    },\n",
      "    \"I\": {\n",
      "        \"A\": 0.2957068421180174,\n",
      "        \"C\": 0.08628578837158679,\n",
      "        \"G\": 0.23937315169824658,\n",
      "        \"T\": 0.37863421781214923\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# This section of code will initialize your HMM with parameters as defined in the lecture slides\n",
    "# for the identification of CpG Islands.\n",
    "# All of this should be able to run whether or not you implement the Viterbi function!\n",
    "\n",
    "hidden_states = ('I', 'G') # CpG Island or Genome\n",
    "alphabet = ('A', 'C', 'G', 'T') # DNA Alphabet\n",
    "\n",
    "model = HMM(alphabet, hidden_states, seed=70)\n",
    "\n",
    "sequence = [\"ACGCGATCATACTATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATGATCGATTACCCCCCCCCCCAATTA\", \"GCAGATCGATCGATATATTAGCTAAATAGATACGCGCGCGCGCGCGATATATGCATATAGCTAATGATGACCCCCGCGCA\", \"ACATCGATCTGATCGAAATAGATACGCGCGCGCGCGCGATATATATATATAGCTAATACTTGATCGATGCAA\"]\n",
    "\n",
    "model.baum_welch(sequence)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After one iteration:\n",
    "```Alphabet: ['A', 'C', 'G', 'T']\n",
    "Hidden States: ['G', 'I']\n",
    "Initial Probabilities: {\n",
    "    \"G\": 0.480482653962524,\n",
    "    \"I\": 0.519517346037476\n",
    "}\n",
    "Transition Probabilities: {\n",
    "    \"G\": {\n",
    "        \"G\": 0.6514755213356082,\n",
    "        \"I\": 0.3485244786643918\n",
    "    },\n",
    "    \"I\": {\n",
    "        \"G\": 0.6035452125544299,\n",
    "        \"I\": 0.39645478744557\n",
    "    }\n",
    "}\n",
    "Emission Probabilities: {\n",
    "    \"G\": {\n",
    "        \"A\": 0.3422919266954332,\n",
    "        \"C\": 0.2811097197581482,\n",
    "        \"G\": 0.1946505595358382,\n",
    "        \"T\": 0.1819477940105804\n",
    "    },\n",
    "    \"I\": {\n",
    "        \"A\": 0.3179475236592407,\n",
    "        \"C\": 0.08356033398812515,\n",
    "        \"G\": 0.23181760304768745,\n",
    "        \"T\": 0.36667453930494665\n",
    "    }\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Work for Baum-Welch Algorithm Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = [{'a':1000, 'b':3000, 'c': 100}, {'a':10, 'b':400, 'c': 1000}, {'a':20, 'b':4, 'c': 10}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [2,3,4,5]\n",
    "sum(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for index, value in enumerate(stats):\n",
    "    # print(index)\n",
    "    test = max(stats[index].values())\n",
    "    final = []\n",
    "    final.append(test)\n",
    "    print(type(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {}\n",
    "state = [\"Hi\", \"Now\", \"Time\"]\n",
    "count = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in state:\n",
    "    test[i] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hi': 20, 'Now': 20, 'Time': 20}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hi': 20}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = {}\n",
    "main = \"Family\"\n",
    "states = [\"Lauth\", \"Smith\"]\n",
    "count = [0.6, 0.7]\n",
    "count2 = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2[main] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Family': {}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in states:\n",
    "    test2[state] = {c for c in count}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Family': {}, 'Lauth': {0.6, 0.7}, 'Smith': {0.6, 0.7}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'A', 'T', 'T', 'C', 'C', 'G', 'G']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = \"AATTCCGG\"\n",
    "list(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = 20 * 40\n",
    "\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = (1 + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
